---
title: "download-viirs"
author: "Matt Callahan"
date: "7/1/2022"
output: html_document
---

## VIIRS download 
I will download annual viirs 8-day files for alaskan waters so I can get started on this shiny app while we're working on getting a viirs pipeline going in AKFIN.

```{r}
library(tidyverse)
library(lubridate)
library(tidync)
library(sf)
library(AKmarineareas)

#download 2022

#east of dateline
download.file(url = "https://coastwatch.pfeg.noaa.gov/erddap/griddap/nesdisVHNSQchlaWeekly.nc?chlor_a%5B(2022-01-01T12:00:00Z):1:(2022-05-28T12:00:00Z)%5D%5B(0.0):1:(0.0)%5D%5B(46):1:(69)%5D%5B(-179.98125):1:(-129)%5D", method = "libcurl", mode="wb",destfile = "Data/viirs_2022_E.nc")

#west of dateline
download.file(url = "https://coastwatch.pfeg.noaa.gov/erddap/griddap/nesdisVHNSQchlaWeekly.nc?chlor_a%5B(2022-01-01T12:00:00Z):1:(2022-05-28T12:00:00Z)%5D%5B(0.0):1:(0.0)%5D%5B(69):1:(46)%5D%5B(167):1:(179.98125)%5D", method = "libcurl", mode="wb",destfile = "Data/viirs_2022_W.nc")

#chla function
tidy_chl<-function(file) {
  tidync(file) %>% 
  hyper_tibble() %>% 
  mutate(date=as_datetime(time),
         year=year(date),
         month=month(date),
         lon_360=ifelse(longitude>0, longitude, longitude+360),
         lonc=as.numeric(ifelse(longitude<0, #ifelse statement keeps +- lons the same length
                                substr(longitude,1,8),
                                substr(longitude,1,7))), 
         latc=as.numeric(substr(latitude,1,6)), 
         chlorophyll=round(chlor_a,3))
}

#east
  viirs_2022_E<-tidy_chl("Data/viirs_2022_E.nc") 
  #west
  viirs_2022_W<-tidy_chl("Data/viirs_2022_W.nc") 
   
   #combine
   viirs_2022<-viirs_2022_E%>% bind_rows(viirs_2022_W)
   rm(viirs_2022_E); rm(viirs_2022_W)

```

## Assign Ecological regions
Since I already made a viirs lookup table this should be a simple left join...
```{r}
#import lookup table. 
#Code on sharefile Personal folders>projects>chlorophyll... 
#maybe I'll eventually move it to a github repo.
lkp<-read.csv("Data/viirs_chl_spatial_lookup.csv")%>%
  dplyr::select(-c(X, latitude, longitude))

#
viirs_2022_esr<-inner_join(viirs_2022, lkp, by=c("lonc"="lonc", "latc"="latc"))
#plot to check
ak<-AK_basemap()
ggplot()+
  geom_point(data=viirs_2022, aes(x=lon_360, y=latitude))+
  geom_point(data=viirs_2022_esr, aes(x=lon_360, y=latitude, color=Ecosystem_Subarea))+
  geom_sf(data=ak%>%st_shift_longitude())
```

## Save

```{r}
#remove some extra fields and save
viirs_2022_esr%>%
  dplyr::select(-c(altitude, time, lonc, latc, chlor_a))%>%
  saveRDS("Data/viirs2022.RDS")

```

## Download past years
Past years are needed to put current conditions into context.

```{r}
#download the rest of the years
options(timeout=6000)
myyear <- 2013:2021
#east of dateline
for(i in myyear){
  file_name <- paste0("Data/viirs_",i,"_e.nc")
  download.file(url = paste0("https://coastwatch.pfeg.noaa.gov/erddap/griddap/nesdisVHNSQchlaWeekly.nc?chlor_a%5B(",
                             i,"-02-01T00:00:00Z):(", i,"-5-28T12:00:00Z)%5D%5B(0.0):1:(0.0)%5D%5B(46):1:(69)%5D%5B(-179.98125):1:(-129)%5D"),
                method = "libcurl", mode="wb",destfile = file_name)
}

#west of dateline
for(i in myyear){
  file_name <- paste0("Data/viirs_",i,"_w.nc")
  download.file(url = paste0("https://coastwatch.pfeg.noaa.gov/erddap/griddap/nesdisVHNSQchlaWeekly.nc?chlor_a%5B(",
                             i,"-02-01T00:00:00Z):(", i,"-5-28T12:00:00Z)%5D%5B(0.0):1:(0.0)%5D%5B(46):1:(69)%5D%5B(167):1:(179.98125)%5D"),
                method = "libcurl", mode="wb",destfile = file_name)
}

```

Load data
```{r}

#create blank list
datalist = list()
#bring in all files with for loop
for (i in myyear){
  dat <- tidy_chl(paste0("Data/viirs_",i,"_e.nc"))
    datalist[[i]] <- dat 
}

#convert list to data frame
viirs_e <- dplyr::bind_rows(datalist)

#create blank list
datalist = list()
#bring in all files with for loop
for (i in myyear){
  dat <- tidy_chl(paste0("Data/viirs_",i,"_w.nc"))
    datalist[[i]] <- dat 
}

#convert list to data frame
viirs_w <- dplyr::bind_rows(datalist)

rm(dat);rm(datalist)  
   #combine
   viirs<-viirs_e%>% bind_rows(viirs_w)
   rm(viirs_e); rm(viirs_w)

```

Join esr info

```{r}

#
viirs_old_esr<-inner_join(viirs, lkp, by=c("lonc"="lonc", "latc"="latc"))
#plot to check
ak<-AK_basemap()
ggplot()+
  geom_point(data=viirs_old_esr, aes(x=lon_360, y=latitude))+
  #geom_point(data=viirs_2022_esr, aes(x=lon_360, y=latitude, color=Ecosystem_Subarea))+
  geom_sf(data=ak%>%st_shift_longitude())

#remove viirs
rm(viirs)
```

Save
```{r}
saveRDS(viirs_old_esr, "Data/viirs_old_esr.RDS")

```

## Calculate averages
Below I filter by depth, distance from shore, and calculate averages and sds. This way I won't have to do this in the app.

```{r}
viirs_old<-readRDS("Data/viirs_old_esr.RDS")

#Limit to after January 28 and define weekday
viirs_old<-viirs_old%>%filter(yday(date)>28)%>%
  mutate(week=week(date))

#ESR regions
#Aleutians filters (no depth)
viirs_ai<-viirs_old%>%filter(Ecosystem_Area=="Aleutian Islands" & WATERS_COD=="FED")
#BSGOA filters (includes depth)
viirs_bsgoa<-viirs_old%>%filter(Ecosystem_Area=="Eastern Bering Sea" |  Ecosystem_Area=="Gulf of Alaska")%>%
  filter(depth < -10 & depth> -200 & WATERS_COD=="FED")

#combine
viirs_old_esr<-viirs_ai%>%bind_rows(viirs_bsgoa)
#remove old ones
rm(viirs_ai);rm(viirs_bsgoa)


####The following code snippets were used to determine whether to plot daily or weekly. 
#test if dates are consistent year to year 
#unique(viirs_old_esr$date)#looks like it!

#looks like I downloaded early january for the western AI...
#viirs_old_esr<-viirs_old_esr%>%filter(yday(date)>28)%>%
#  mutate(doy=yday(date))

#check yday
#unique(viirs_old_esr$doy)#looks like it!
#all good until 72 and 147...
#unique((viirs_old_esr%>%filter(doy==72))$year)
#unique((viirs_old_esr%>%filter(doy==147))$year)

#what if I do it by week
#viirs_old_esr<-viirs_old_esr%>%
#  mutate(week=week(date))
  
#  table(viirs_old_esr$doy, viirs_old_esr$year)
#    table(viirs_old_esr$doy, viirs_old_esr$week)
    
    #it looks like in 2018 data is from day 72 rather than 71,
    #and in 2021 data is from day 147 instead of 141 (and then again for day 148 for the next week!)
    #I am going to plot weekly averages.
    #First I will create a weekly average by year, then average those averages.

#Average by ESR region
#It looks like it is desirable to plot by year. 
viirs_old_avg_esr<-viirs_old_esr%>%
  mutate(region=Ecosystem_Subarea) %>% 
  group_by(region, year, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T)) #%>%
  #group_by(Ecosystem_Subarea, week)%>%
  #summarise(chlorophyll=mean(chla, na.rm=T),
            #stdev=sd(chla, na.rm=T))

#and add BSIERP regions
viirs_old_bsierp<-viirs_old%>%filter(BSIERP_ID>0 & WATERS_COD=="FED")
viirs_old_avg_bsierp<-viirs_old_bsierp%>%
  mutate(region=BSIERP_Region_Name) %>% 
  group_by(region, year, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T))

#combine and save
viirs_old_avg_esr%>%bind_rows(viirs_old_avg_bsierp)%>%saveRDS("Data/viirs_old_avg.RDS")
```

Now I should do the same calcs with 2022

```{r}
viirs_2022<-readRDS("Data/viirs2022.RDS")

#test if dates are consistent year to year 
unique(viirs_2022$date)#looks like it!

#looks like I downloaded early january for the western AI...
viirs_2022<-viirs_2022%>%filter(yday(date)>28)%>%
  mutate(week=week(date))

#ESR regions
#Aleutians filters (no depth)
viirs_ai<-viirs_2022%>%filter(Ecosystem_Area=="Aleutian Islands" & WATERS_COD=="FED")
#BSGOA filters (includes depth)
viirs_bsgoa<-viirs_2022%>%filter(Ecosystem_Area=="Eastern Bering Sea" |  Ecosystem_Area=="Gulf of Alaska")%>%
  filter(depth < -10 & depth> -200 & WATERS_COD=="FED")

#combine
viirs_2022_esr<-viirs_ai%>%bind_rows(viirs_bsgoa)
#remove old ones
rm(viirs_ai);rm(viirs_bsgoa)

#averages
viirs_2022_avg_esr<-viirs_2022_esr%>%
  mutate(region=Ecosystem_Subarea)%>%
  group_by(region, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T),
            n=n())

#BSIERP regions
#and add BSIERP regions
viirs_2022_bsierp<-viirs_2022%>%filter(BSIERP_ID>0 & WATERS_COD=="FED")
viirs_2022_avg_bsierp<-viirs_2022_bsierp%>%
  mutate(region=BSIERP_Region_Name)%>%
  group_by(region, year, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T),
            n=n())
#Combine and save
viirs_2022_avg_esr%>%bind_rows(viirs_2022_avg_bsierp)%>%saveRDS("Data/viirs_2022_avg.RDS")
```

New metrics will be the Southeastern Bering Sea shelf, EGOA, and WGOA
```{r}
#old
#2022 GOA
viirs_old_avg_goa<-viirs_old_avg_esr%>%
  filter(region %in% c("Eastern Gulf of Alaska", "Western Gulf of Alaska"))
#BS
viirs_old_avg_bs<-viirs_old_esr%>%
  filter(BSIERP_ID %in% c(1,3,5,6) & WATERS_COD=="FED")%>%
  mutate(region="SEBS Shelf")%>%
  group_by(region, year, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T),
            n=n())

#combine and save
viirs_old_avg_goa%>%bind_rows(viirs_old_avg_bs)%>%saveRDS("Data/viirs_old_bsgoa_avg.RDS")

#2022 GOA
viirs_2022_avg_goa<-viirs_2022_avg_esr%>%
  filter(region %in% c("Eastern Gulf of Alaska", "Western Gulf of Alaska"))
#BS
viirs_2022_avg_bs<-viirs_2022_esr%>%
  filter(BSIERP_ID %in% c(1,3,5,6) & WATERS_COD=="FED")%>%
  mutate(region="SEBS Shelf")%>%
  group_by(region, week)%>%
  summarise(chlorophyll=mean(chlorophyll, na.rm=T),
            n=n())

#combine and save
viirs_2022_avg_goa%>%bind_rows(viirs_2022_avg_bs)%>%saveRDS("Data/viirs_2022_bsgoa_avg.RDS")
```

